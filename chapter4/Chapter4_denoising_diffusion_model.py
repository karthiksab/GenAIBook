# -*- coding: utf-8 -*-
"""Denoising_Diffusion_Model_23_10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PBPVXqGtZDDbapPTIrPkEdIhDG9ux6qG

#Import libraries
"""

import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from keras.datasets.mnist import load_data
# import unet.py file into current foler
from unet import UNet
import matplotlib.pyplot as plt

"""#Download Mnist dataset"""

(trainX, trainy), (testX, testy) = load_data()
trainX = np.float32(trainX) / 255.
testX = np.float32(testX) / 255.

def sample_images(batch_size, device):
    indices = torch.randperm(trainX.shape[0])[:batch_size]
    data = torch.from_numpy(trainX[indices]).unsqueeze(1).to(device)
    return torch.nn.functional.interpolate(data, 32)

"""# DDM Modle build


"""

class DenoisingDiffusionModel():

    def __init__(self, T : int, model : nn.Module, device : str):

        self.T = T
        self.UNet_Model = model.to(device)
        self.device = device
        self.beta = torch.linspace(1e-4, 0.02, T).to(device)
        self.alpha = 1. - self.beta
        self.alpha_bar = torch.cumprod(self.alpha, dim=0)

    def training(self, batch_size, optimizer):

        x0 = sample_images(batch_size, self.device)

        t = torch.randint(1, self.T + 1, (batch_size,), device=self.device, dtype=torch.long)

        epsilon = torch.randn_like(x0)

        # Take one gradient descent step
        alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
        predicted_epsilon = self.UNet_Model(torch.sqrt(
            alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * epsilon, t-1)
        loss = nn.functional.mse_loss(epsilon, predicted_epsilon)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        return loss.item()

    @torch.no_grad()
    def sampling(self, num_samples=1, img_chn=1, img_size=(32, 32)):

        x = torch.randn((num_samples, img_chn, img_size[0], img_size[1]),
                         device=self.device)

        #progress_bar = tqdm if use_tqdm else lambda x : x
        for t in (range(self.T, 0, -1)):
            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)

            t = torch.ones(num_samples, dtype=torch.long, device=self.device) * t

            beta_t = self.beta[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
            alpha_t = self.alpha[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
            alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)

            mean = 1 / torch.sqrt(alpha_t) * (x - ((1 - alpha_t) / torch.sqrt(
                1 - alpha_bar_t)) * self.UNet_Model(x, t-1))
            sigma = torch.sqrt(beta_t)
            x = mean + sigma * z

        return x

"""# Training of Model"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 64
model = UNet()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
ddm = DenoisingDiffusionModel(1000, model, device)

train_loss = []
for epoch in range(1000):
    loss = ddm.training(batch_size, optimizer)
    print(loss)
    train_loss.append(loss)

"""# Output"""

input_images=100
samples = ddm.sampling(num_samples=input_images)
plt.figure(figsize=(17, 17))
for i in range(input_images):
    plt.subplot(10, 10, 1 + i)
    plt.axis('off')
    plt.imshow(samples[i].squeeze(0).clip(0, 1).data.cpu().numpy(), cmap='gray')
plt.savefig(f'epoch_{epoch}.png')
plt.close()

torch.save(model.cpu(), f'DDM_epoch_{epoch}')